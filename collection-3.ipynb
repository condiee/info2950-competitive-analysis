{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, I scrape the Mayo Clinic's Symptoms and Causes pages under all of their indexed diseases and conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import sys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from string import ascii_uppercase as upp\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get the long list of symptoms-causes URLs that are linked from the Mayo Clinic's indexed Diseases and Conditions lookup. I begin by saving the url up to and including the query for the letter, but not the letter itself. I also save the root of the URL for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get URLs: diseases-conditions pages organized in an alphabetical index, with one # entry\n",
    "url = 'https://www.mayoclinic.org/diseases-conditions/index?letter='\n",
    "root = 'https://www.mayoclinic.org'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I write a function to extract the URLs of the pages I want from the letter index pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(letter, addresses):\n",
    "    index = url + letter # url of index page for this letter\n",
    "    get = requests.get(index).content\n",
    "    soup = BeautifulSoup(get, \"lxml\")\n",
    "    # get list of articles on index page\n",
    "    within = soup.find_all(class_ = \"index content-within\")\n",
    "    for elm in within:\n",
    "        # gets letter's articles as strings in a list\n",
    "        links = re.findall(\"(?<=a\\shref=\\\").*?(?=\\\">)\", str(elm))\n",
    "        # for each of the links\n",
    "        for page in range(len(links)): \n",
    "            full = root + links[page] # symptoms-causes URL\n",
    "            if addresses.count(full)<1:\n",
    "                addresses.append(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the function to extract the URL from the # index page, the only index item that isn't listed under a capital letter of the alphabet. This helps me to ensure the function is behaving correctly without scraping too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = []\n",
    "extract(\"0\", addresses)\n",
    "addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I know it works, I can extract the other URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in upp:\n",
    "    extract(letter, addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to put these URLs into an initial dataframe that I will merge with the relevant Spider and Moz data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mayo_data = pd.DataFrame(addresses, columns=[\"url\"])\n",
    "mayo_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mayo_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: SEO Spider Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Source Data: https://drive.google.com/open?id=1vlTTVOf3L2TnJxRma4TyJPCsgKpVvM19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are all of the columns provided by SEO Spider, not all of which will be useful for my purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw = pd.read_csv(\"symptoms-causes.csv\")\n",
    "raw.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I select which columns I want to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed = raw[[\"URL Encoded Address\", 'H1-1', 'H1-1 length', \"Meta Description 1\", 'Meta Description 1 Length', 'Size (bytes)', \"Word Count\", \"Inlinks\", \"Unique Inlinks\", 'Outlinks', 'Unique Outlinks', 'External Outlinks',\n",
    "       'Unique External Outlinks']]\n",
    "trimmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and convert them into more coding-friendly formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lowercase, spaces to underscores\n",
    "new_colnames = [x.lower() for x in trimmed.columns]\n",
    "new_colnames = [x.replace(' ', '_') for x in new_colnames]\n",
    "\n",
    "# replace in original dataframe\n",
    "cleaned = trimmed\n",
    "cleaned.columns = new_colnames\n",
    "\n",
    "# replace individual column names that need modifying\n",
    "cleaned = cleaned.rename(columns = {'url_encoded_address' : 'url'})\n",
    "cleaned = cleaned.rename(columns = {'h1-1': 'header'})\n",
    "cleaned = cleaned.rename(columns = {'h1-1_length': 'header_len'})\n",
    "cleaned = cleaned.rename(columns = {'meta_description_1' : 'meta'})\n",
    "cleaned = cleaned.rename(columns = {'meta_description_1_length' : 'meta_len'})\n",
    "cleaned = cleaned.rename(columns = {'size_(bytes)' : 'bytes'})\n",
    "cleaned = cleaned.rename(columns = {'unique_inlinks' : 'unique_in'})\n",
    "cleaned = cleaned.rename(columns = {'unique_outlinks' : 'unique_out'})\n",
    "cleaned = cleaned.rename(columns = {'external_outlinks' : 'ext_links'})\n",
    "cleaned = cleaned.rename(columns = {'unique_external_outlinks' : 'unique_ext'})\n",
    "\n",
    "# reformat headers\n",
    "lower = [x.lower() for x in cleaned[\"header\"]]\n",
    "cleaned[\"header\"] = lower\n",
    "\n",
    "cleaned.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inner merge to get all URLs scraped from Mayo Clinic site that were included in the SEO Spider crawl\n",
    "data = pd.merge(mayo_data, cleaned, on='url', how='inner')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for any duplicate URLs\n",
    "data = data[data[\"url\"].duplicated() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no duplicates, but we did lose some URLs that were scraped from the Mayo Clinic site but weren't accessed in the crawl.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check for any NaNs in the dataset\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check where\n",
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which it is\n",
    "data[data[\"meta\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set null meta description to empty instead of null (without making a copy)\n",
    "data.loc[data['meta'].isnull(), 'meta'] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"header\"]==\"tapeworm infection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that was it\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Moz Data\n",
    "### Top 500 Ranking Pages on the Mayo Clinic Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moz = pd.read_csv(\"moz-top-pages.csv\")\n",
    "moz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase, spaces to underscores\n",
    "colnames = [x.lower() for x in moz.columns]\n",
    "colnames = [x.replace(' ', '_') for x in colnames]\n",
    "\n",
    "# replace in original dataframe\n",
    "moz.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure url format matches\n",
    "urls = []\n",
    "for row in range(len(moz[\"url\"])):\t\n",
    "    urls.append(\"https://\"+moz[\"url\"][row])\n",
    "\n",
    "moz[\"url\"]=urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many pages we keep if we merge this data with the larger dataset of symptoms and causes pages\n",
    "experiment = pd.merge(data, moz, on='url', how='inner')\n",
    "experiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of any duplicate URLs\n",
    "experiment = experiment[experiment[\"url\"].duplicated() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# by merging the datasets, and including only top ranking pages, I've lost a lot of data\n",
    "experiment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the range of the page authority variable that I wanted\n",
    "experiment['pa'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60-67 is something, since these rankings are logarithmic\n",
    "# and therefore it's harder to increase page rank as you near 100,\n",
    "experiment['pa'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but the variation between successful pages is wider than it seems\n",
    "experiment['pa'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than half of the 500 top-ranking pages on the Mayo Clinic website are Symptoms and Causes pages. Of those pages, the range of Page Authority ranking out of 100 is from 60-67, not very wide. This is a very small sample with not much variance, and I have similar data already from SEO Spider, so I will use the data from Moz about Page Authority in a different way. What I want to know is which pages in my dataset are in the top 500, and since the Page Authority scores of those pages are fairly close together, I will mark them as top-ranking with a 1 (True), and the rest of the pages in the dataset with a 0 (False). This way I am not losing very much information, and I am keeping my sample size much closer to the population size.\n",
    "\n",
    "It would be most desirable if I had the page authority ranking from Moz for all of the Mayo Clinic pages I am interested in looking at, but unfortunately they only provide the top 500 for the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller dataset with page authority\n",
    "top_data = experiment[[\"url\", 'pa', 'header', 'header_len', 'meta', 'meta_len', 'bytes',\n",
    "       'word_count', 'total_links', 'linking_domains_to_page', 'ext_links', 'unique_ext',\n",
    "        'inlinks', 'unique_in', 'outlinks', 'unique_out']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column\n",
    "top_data = top_data.rename(columns = {'linking_domains_to_page' : 'inbound_domains'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: More Web Scraping (publication date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dates = []\n",
    "\n",
    "for page in top_data[\"url\"]:\n",
    "    content = requests.get(page).content # page content\n",
    "    file = BeautifulSoup(content, \"lxml\") # in lxml\n",
    "    date = file.find(\"div\", class_='pubdate')\n",
    "    if(date!=None):\n",
    "        match = re.findall(\"(?<=\\\\r\\\\n).*?(?=\\\\r\\\\n)\", str(date.get_text()))[0].strip() # get rid of \\r\\n and spaces\n",
    "        dates.append(match) # add to column list\n",
    "    else:\n",
    "        dates.append(None)\n",
    "\n",
    "# add new column to dataframe\n",
    "top_data['pub_date'] = dates\n",
    "top_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any didn't get scraped\n",
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert strings to comparable datetime objects\n",
    "import datetime\n",
    "# jan, feb, aug, sept, oct, nov, dec\n",
    "dat = []\n",
    "for date in top_data[\"pub_date\"]:\n",
    "    if type(date)==str: # if it isn't null\n",
    "        if \".\" in date:\n",
    "            if \"Sept\" in date: #special case: datetime recognizes \"Sep\" not \"Sept\"\n",
    "                date = re.sub('t', '', date)\n",
    "            datetime_ob = datetime.datetime.strptime(date, '%b. %d, %Y')\n",
    "        else:\n",
    "            datetime_ob = datetime.datetime.strptime(date, '%B %d, %Y')\n",
    "        dat.append(datetime_ob)\n",
    "    else:\n",
    "        dat.append(None)\n",
    "\n",
    "top_data.pub_date = dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_data.to_csv('ranked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
